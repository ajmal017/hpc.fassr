\chapter{Parallelization}
\label{chap:parallelization}

\section{PyCOMPSs overview}

PyCOMPSs \cite{pycompss} is a framework aimed to ease the development of parallel and distributed Python applications. The primary purpose is to abstract users from both infrastructure management and data handling. PyCOMPSs is composed of two main parts: the programming model and the runtime. The programming model provides a set of simple annotations to indicate which functions can be run in parallel. The runtime handles the data dependencies between the tasks and distributes the computation among the available resources.

PyCOMPSs code is portable because it is infrastructure unaware and it can be run in a wide number of platforms, such as clouds and grids while providing a uniform interface for the user. 

\subsection{Programming model}
% TODO cite openMP etc...
PyCOMPSs offers a sequential programming model to specify the parts of the code that can be run in parallel. This differs from other models and paradigms that require the developer to have a deep knowledge of the hardware executing the code such as MPI interfaces or OpenMP pragmas. 

To indicate which parts of the code execution can be distributed, PyCOMPSs takes advantage of the python function decorators~\cite{decorators}. The basic PyCOMPSs decorator is \textit{@task()} and it states that a given function should be treated as a task. 

Tasks are run distributed, and their parameters need to be instrumented to transfer them between the nodes. All the inputs and outputs of tasks are placeholders to allow them to run asynchronously while the main code continues to be executed. When the value of some return is needed, we need to synchronize it to the master using the API call \textit{compss\_wait\_on}. 

Figure~\ref{fig:annotation} shows an example application that computes the sum of the squares of the numbers in the list $[0, 10]$. The \textit{multiply} function is decorated as a task and all of them are thus run distributed. The output of these tasks is then passed to the \textit{mean} function which computes the average. Finally, we synchronize the value and print it.


\begin{figure}[t!]
\centering
\begin{minted}{python}
from pycompss.api.task import task
from pycompss.api.api import compss_wait_on

@task(returns=int)
def multiply(num1, num2):
    return num1 * num2

@task(returns=float)
def mean(*numbers):
    return sum(numbers) / len(numbers)

s = [multiply(i, i)) for i in range(10)]
avg = compss_wait_on(mean(*s))
print(avg)
\end{minted}
\caption{PyCOMPSs application example that squares each number in $[0, 10)$ and then computes the mean.}
\label{fig:annotation}
\end{figure}


\subsection{Runtime}

The PyCOMPSs framework is mostly a set of bindings to interact with the Java runtime. The runtime has two primary responsibilities: analyzing tasks' dependencies and scheduling them, and resources and data management.

To execute in distributed the functions decorated as tasks, the runtime needs to compute the data dependencies between them. That information is then used to build a task dependencies graph which determines the order of the execution (e.g., if a task $A$ generates the input of a task $B$, then $B$ cannot be run until $A$ is finished). Once tasks are free of dependencies in the graph, they are scheduled to be executed in some of the available computing resources.

Concerning resources management, the runtime follows a master-worker paradigm. All available computing resources are represented uniformly as workers. The master node is responsible for executing the user code. When it encounters a task, the task is scheduled (following the dependencies graph) to a given worker, its required input is transferred, and the results, if any, are gathered and sent back.

Thanks to the pluggable connectors PyCOMPSs code is infrastructure-agnostic and can be run in a wide number of platforms without requiring any change. These include clouds, cluster and grid nodes, or docker/singularity containers, using connectors like Slurm~\cite{slurm} or Apache Mesos~\cite{mesos}. Furthermore, PyCOMPSs supports heterogeneous architectures allowing the user to mix GPUs and FPGAs~\cite{amela17} with traditional CPU computing resources.

\subsection{Tools}

PyCOMPSs also offers some useful tools for execution analysis: the monitor and the tracing system. The monitoring offers information about executions such as diagrams of data dependencies, resources state details, statistics and easy access to the framework logs. The tracing system uses Extrae~\cite{extrae} to  generate post-mortem execution trace files. The trace files can be analyzed with the graphical tool Paraver~\cite{paraver}.

Figure~\ref{fig:trace_example} shows an example of a tracefile from Figure~\ref{fig:annotation}. This view shows only the executed tasks. Each horizontal bar represents a thread of the application. It is worth noting, that despite being called \textit{threads}, each line corresponds to an independent python process, and that there are as many python processes as CPUs. The threads are named \textit{Thread x.y.z} where $x$ is the application the thread belongs to, the $y$ is the node, and the $z$ is the thread number in the node. In this example, we have two nodes 1.1 and 1.2. The first three threads (1.1.1 - 1.1.3) correspond to the master node. The other five threads belong to a single worker with four computing units (1.2.2 - 1.2.5). The first thread of each worker (1.2.1) is used to show data transfers and other worker internal events. Both the master node and the worker's main thread are empty because they do not execute any task directly.

Figure~\ref{fig:graph_example} shows the data dependencies graph of the sample application from Figure~\ref{fig:annotation}. The input arrows mark the data dependencies between tasks. All the multiply tasks can be run in parallel as they only depend on their input parameter. The mean task depends on all the objects of the list.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{images/local_trace_example.png}
    \caption{\small Tracefile of the sample application from Figure~\ref{fig:annotation}.}
    \label{fig:trace_example}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=10cm]{images/local_graph.png}
    \caption{\small Task dependencies graph of the sample application from Figure~\ref{fig:annotation}.}
    \label{fig:graph_example}
\end{figure}


\section{Hardware}

To test the parallelization efficiency, we run the experiments of the project in the MareNostrum 4 supercomputer~\cite{mn4}. 

MareNostrum 4 has 3456 computing nodes housing a total of 165.888 cores, 390 Terabytes of main memory, and a peak performance of 11.15 Petaflops. Each node contains:

\begin{itemize}
\item 2 Intel Xeon Platinum 8160 CPU with 24 cores each at 2.10 GHz
\item 96 GB of main memory
\item 200 GB of local SSD storage.
\item L1d 32K; L1i cache 32K; L2 cache 1024K; L3 cache 33792K
\item 100 Gbit/s Intel Omni-Path HFI Silicon 100 Series PCI-E adapter
\item 10 Gbit Ethernet
\end{itemize}
 
% MareNostrum 4 has a total disk storage capacity of 14 Pb and is connected to the European research network. 
\section{Implementation}

\HPCsys should be able to easily explore and train a considerable number of ML models, parametrizations, and inputs in a relatively short amount of time. To speed up the computing we distributed the execution of the stages of \HPCsys's pipeline (see Figure~\ref{fig:pipeline}) \textit{Download Input Data}, \textit{Data Preprocessing}, \textit{Model Training}, and \textit{Trading}.  All \HPCsys code is available at Github~\footnote{http://github.com/kafkasl/hpc.fassr}

The \textit{Download Input Data} is parallelized at a very low-level. Downloading all the data through an API is slow, hitting at times 50K API calls. To overcome that, we do each API call in a task. %% add the monitor graph etc... of an example
Intrinio has a daily API limit. To avoid decrease the number of requests, all API calls are cached locally to avoid duplicated calls. These cached results are shared across executions.

For the \textit{Data Preprocessing} stage, the two tasks \textit{get\_fundamentals} and \textit{get\_prices} build the fundamentals dataset and the historical prices respectively. 

Next, we tried parallelizing the features and target building of each symbol individually. However, in our tests with the S\&P 500, the time of processing the 500 symbols is not large enough to warrant parallelization and all symbols are processed together in a single \textit{process\_symbols} task. Figure~\ref{fig:process_symbols} shows the data dependencies graph when processing all symbols together or one in each task.

Finally, the \textit{post\_process} task creates the z-scores if needed, filters some invalid values, and caches the dataset to disk for reuse. 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{images/processing_symbols.png}
    \caption{\small Task dependencies graphs for the versions where all the symbols are processed in a single task (left) and the one where each symbol is processed in a different task (right).}
    \label{fig:process_symbols}
\end{figure}

The \textit{Model Training} and \textit{Trading} are grouped into four types of different tasks:
\begin{enumerate}
    \item \textit{run\_graham} 
    \item \textit{run\_classification\_model}
    \item \textit{run\_regression\_model}
    \item \textit{run\_parallel\_model}
\end{enumerate}

The first task evaluates Graham's criteria. The second and third tasks evaluate classification  and regression models respectively. We differentiated between regression and classification models only for visualization purposes. Both are run identically; only the input data is different.
If the target model supports intra-node parallelization, defined by the \textit{n\_jobs} parameter, we execute the fourth task with hardware constraints equal to number \textit{n\_jobs}. We explored different values for \textit{n\_jobs} and the best times were obtained setting it to use a quarter of the node: $\text{n\_jobs} = 12~\text{CPUs}$.

\section{Checkpointing}


We aim to run very large executions so we developed a checkpointing system to be able to reuse data from a finished execution or another currently running one. Our aim is twofold: on the one hand, this allows users to reuse data from failed executions, and, on the other, to reuse data from overlapping configurations across different executions.

The strategy followed is to cache the intermediate trained models into disk. For each configuration, the \textit{run} tasks train as many models as trading sessions exist. This is the most time-consuming part of the application so every time one of the tasks finishes training the model of a session, the model is saved to disk. The name for the file is a hash derived of the configuration that is being evaluated and the trading session. 
This naming convention allows other executions to reuse the model directly if they use the same cache directory. If the cache directory is in a shared file system, all other executions can reuse it. Otherwise, the models will be only usable by executions on the same node.

Thanks to this, if an application is stopped during the execution (e.g., canceled due to time limit by the queuing system in supercomputers), rerunning it will reuse all the models and continuing where it left off. Also, in executions with overlapping configurations (e.g., one execution trading each year, the other each semester), each model will be trained by the first execution and reused by the others. We do not implement any contention policy because the probability of two configurations looking for the same file simultaneously is negligible in our tests.

The amount of space required to cache big executions is not trivial. In order to keep it to a minimum, all models are compressed with \textit{gzip} python package after being serialized.

\section{Evaluation}

We evaluate the performance of \HPCsys under strong and weak scaling conditions. Strong scaling measures how the execution time changes with the number of CPUs for a fixed problem size. On the other hand, weak scaling measures the execution for different numbers of CPUs when the problem size is fixed \textit{per CPU} (i.e., the total problem size increases with the number of CPUs). 

For the strong scaling tests, we use a sample execution of 4000K configurations. For the weak scaling, we use a ratio of $ \frac{25}{24} \text{ configurations/CPU}$ to match our smallest execution of a single node with 48 CPUs. To keep the workload constant, we use a base sample of 10 configurations and replicate it to obtain sample sizes in $[50, 150, 350, 750, 1550, 3150]$.

In both scenarios, we run our tests with 1, 2, 4, 8, 16, and 32 nodes. Each node has 48 CPUs except when using a single node which uses only 24 CPUs because half of the node is used by the worker, the other half by the master. MareNostrum 4 supercomputer does not allow outbound internet connections, so the executions were done using the data cached by the section \textit{Download Input Data} from Figure~\ref{fig:pipeline}.

For each test, we report the execution time of the python application. The speedup of a given application $a$ and a base application $b$ is computed as $ S_{a, b} = \frac{t_{a}}{t_{b}} $, where $ t_x=\text{execution time of application}\ x $. 